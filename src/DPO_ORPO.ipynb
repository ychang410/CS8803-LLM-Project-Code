{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Prep**"
      ],
      "metadata": {
        "id": "ZPz6VgN2ZizK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M26g8jIAF_qC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets trl accelerate bitsandbytes huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/EleutherAI/lm-evaluation-harness.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "y3l0sBtxGIVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate bitsandbytes langdetect"
      ],
      "metadata": {
        "id": "50WDoHCqGJ3R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "DTa_ElV7GvaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Datasets**"
      ],
      "metadata": {
        "id": "H7i8A2RWZmdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset2 = load_dataset('llmf25/shuffled_dpo')\n",
        "print(dataset2)"
      ],
      "metadata": {
        "id": "Fd4tvnI_GRR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Models**"
      ],
      "metadata": {
        "id": "ftLSSPrwZrPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DPO**"
      ],
      "metadata": {
        "id": "YbgWka3ZaMDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "# Single: \"Qwen/Qwen2.5-0.5B\"\n",
        "# Hybrid: \"llmf25/qwen2_5_0_5b_llmf25_sft_mini\"\n",
        "model_name = \"llmf25/qwen2_5_0_5b_llmf25_sft_mini\"\n",
        "output_dir2 = \"./hybrid_dpo\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.bfloat16, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "RG9v3sF1GTts",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dpo_examples(sample):\n",
        "    user_question = f\"{sample['prompt']}\"\n",
        "\n",
        "    chosen_messages = [\n",
        "        {\"role\": \"user\", \"content\": user_question},\n",
        "        {\"role\": \"assistant\", \"content\": sample['chosen']}\n",
        "    ]\n",
        "    chosen_formatted = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "\n",
        "    rejected_messages = [\n",
        "        {\"role\": \"user\", \"content\": user_question},\n",
        "        {\"role\": \"assistant\", \"content\": sample['rejected']}\n",
        "    ]\n",
        "    rejected_formatted = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "\n",
        "    return {\n",
        "        'chosen': chosen_formatted,\n",
        "        'rejected': rejected_formatted\n",
        "    }\n",
        "\n",
        "train_dataset2 = dataset2['train'].map(\n",
        "    format_dpo_examples\n",
        ")\n",
        "\n",
        "print(\"Formatted train_dataset:\")\n",
        "print(train_dataset2)"
      ],
      "metadata": {
        "id": "7YIdYdsPGaQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DPO config\n",
        "dpo_config = DPOConfig(\n",
        "    output_dir=output_dir2,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=1,\n",
        "    max_length=1024,\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    remove_unused_columns=False,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.03,\n",
        "    warmup_ratio=0.1,\n",
        "    report_to=\"none\",\n",
        "    beta=0.1,\n",
        "    bf16=True if torch.cuda.is_available() else False,\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=dpo_config,\n",
        "    train_dataset=train_dataset2,\n",
        "    processing_class=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(output_dir2)\n",
        "tokenizer.save_pretrained(output_dir2)"
      ],
      "metadata": {
        "id": "4UCpCzuSGcmK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir2, dtype=torch.bfloat16, trust_remote_code=True)\n",
        "print(f\"Model loaded from {output_dir2}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "print(f\"Tokenizer loaded from {model_name}\")\n",
        "\n",
        "model.push_to_hub(\"llmf25/hybrid_sft_dpo\")\n",
        "print(\"Model pushed to Hugging Face Hub: llmf25/hybrid_sft_dpo\")\n",
        "\n",
        "tokenizer.push_to_hub(\"llmf25/hybrid_sft_dpo\")\n",
        "print(\"Tokenizer pushed to Hugging Face Hub: llmf25/hybrid_sft_dpo\")"
      ],
      "metadata": {
        "id": "e499tCNDXupz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ORPO**"
      ],
      "metadata": {
        "id": "u7-eZldAaUDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import Dataset\n",
        "from trl import ORPOTrainer, ORPOConfig\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "id": "cnkQzO43HRWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single: \"Qwen/Qwen2.5-0.5B\"\n",
        "# Hybrid: \"llmf25/qwen2_5_0_5b_llmf25_sft_mini\"\n",
        "\n",
        "model_name = \"llmf25/qwen2_5_0_5b_llmf25_sft_mini\"\n",
        "output_dir_orpo = \"./hybrid_orpo\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.bfloat16, trust_remote_code=True)\n",
        "\n",
        "print(f\"Model and tokenizer loaded for {model_name}.\")"
      ],
      "metadata": {
        "id": "pyKFNgvRHcSS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_orpo_examples(sample):\n",
        "    user_question = f\"{sample['prompt']}\"\n",
        "\n",
        "    chosen_messages = [\n",
        "        {\"role\": \"user\", \"content\": user_question},\n",
        "        {\"role\": \"assistant\", \"content\": sample['chosen']}\n",
        "    ]\n",
        "    chosen_formatted = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "\n",
        "    rejected_messages = [\n",
        "        {\"role\": \"user\", \"content\": user_question},\n",
        "        {\"role\": \"assistant\", \"content\": sample['rejected']}\n",
        "    ]\n",
        "    rejected_formatted = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "\n",
        "    return {\n",
        "        'chosen': chosen_formatted,\n",
        "        'rejected': rejected_formatted\n",
        "    }\n",
        "\n",
        "train_dataset_orpo = dataset2['train'].map(\n",
        "    format_orpo_examples\n",
        ")\n",
        "\n",
        "print(\"Formatted train_dataset_orpo:\")\n",
        "print(train_dataset_orpo)"
      ],
      "metadata": {
        "id": "ccnz1Un4How8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orpo_config = ORPOConfig(\n",
        "    output_dir=output_dir_orpo,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=1,\n",
        "    max_length=1024,\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    remove_unused_columns=False,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.03,\n",
        "    warmup_ratio=0.1,\n",
        "    report_to=\"none\",\n",
        "    beta=0.1,\n",
        "    bf16=True if torch.cuda.is_available() else False,\n",
        ")\n",
        "\n",
        "print(\"ORPOConfig initialized successfully.\")"
      ],
      "metadata": {
        "id": "dRWlg8uEHp18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = ORPOTrainer(\n",
        "    model=model,\n",
        "    args=orpo_config,\n",
        "    train_dataset=train_dataset_orpo,\n",
        "    processing_class=tokenizer\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(orpo_config.output_dir)\n",
        "tokenizer.save_pretrained(orpo_config.output_dir)\n",
        "\n",
        "print(f\"ORPO model trained and saved to {orpo_config.output_dir}.\")"
      ],
      "metadata": {
        "id": "GeYWEL47HtFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir_orpo, dtype=torch.bfloat16, trust_remote_code=True)\n",
        "print(f\"Model loaded from {output_dir_orpo}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "print(f\"Tokenizer loaded from {model_name}\")\n",
        "\n",
        "model.push_to_hub(\"llmf25/hybrid_sft_orpo\")\n",
        "print(\"Model pushed to Hugging Face Hub: llmf25/hybrid_sft_orpo\")\n",
        "\n",
        "tokenizer.push_to_hub(\"llmf25/hybrid_sft_orpo\")\n",
        "print(\"Tokenizer pushed to Hugging Face Hub: llmf25/hybrid_sft_orpo\")"
      ],
      "metadata": {
        "id": "6ngXNVIsHvpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "J1QGMDgaaWyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=llmf25/hybrid_sft_orpo,dtype=bfloat16,trust_remote_code=True \\\n",
        "  --tasks medqa_4options \\\n",
        "  --device cuda:0 \\\n",
        "  --num_fewshot 0 \\\n",
        "  --batch_size auto \\\n",
        "  --output_path dpo_results/medqa.jsonl\\\n",
        "  --log_samples"
      ],
      "metadata": {
        "id": "psRzjdkQWH2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=llmf25/hybrid_sft_orpo,dtype=bfloat16,trust_remote_code=True \\\n",
        "  --tasks headqa_en \\\n",
        "  --device cuda:0 \\\n",
        "  --num_fewshot 0 \\\n",
        "  --batch_size auto \\\n",
        "  --output_path orpo_results/headqa.jsonl\\\n",
        "  --log_samples"
      ],
      "metadata": {
        "id": "H1RUyVgvmfuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=llmf25/hybrid_sft_dpo,dtype=bfloat16,trust_remote_code=True \\\n",
        "  --tasks medqa_4options \\\n",
        "  --device cuda:0 \\\n",
        "  --num_fewshot 0 \\\n",
        "  --batch_size auto \\\n",
        "  --output_path dpo_results/medqa_dpo.jsonl\\\n",
        "  --log_samples"
      ],
      "metadata": {
        "id": "QFUqW2P4WC-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=llmf25/hybrid_sft_dpo,dtype=bfloat16,trust_remote_code=True \\\n",
        "  --tasks medmcqa \\\n",
        "  --device cuda:0 \\\n",
        "  --num_fewshot 0 \\\n",
        "  --batch_size auto \\\n",
        "  --output_path dpo_results/medmcqa.jsonl\\\n",
        "  --log_samples"
      ],
      "metadata": {
        "id": "FK-b-EkWWfE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=llmf25/hybrid_sft_dpo,dtype=bfloat16,trust_remote_code=True \\\n",
        "  --tasks headqa_en \\\n",
        "  --device cuda:0 \\\n",
        "  --num_fewshot 0 \\\n",
        "  --batch_size auto \\\n",
        "  --output_path dpo_results/headqa.jsonl\\\n",
        "  --log_samples"
      ],
      "metadata": {
        "id": "X4waeoe1WjQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=Qwen/Qwen2.5-0.5B,dtype=bfloat16,trust_remote_code=True \\\n",
        "  --tasks headqa_en \\\n",
        "  --device cuda:0 \\\n",
        "  --num_fewshot 0 \\\n",
        "  --batch_size auto \\\n",
        "  --output_path results/headqa.jsonl\\\n",
        "  --log_samples"
      ],
      "metadata": {
        "id": "82xL2nsdoyYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=Qwen/Qwen2.5-0.5B,dtype=bfloat16,trust_remote_code=True \\\n",
        "  --tasks medqa_4options \\\n",
        "  --device cuda:0 \\\n",
        "  --num_fewshot 0 \\\n",
        "  --batch_size auto \\\n",
        "  --output_path results/medqa.jsonl\\\n",
        "  --log_samples"
      ],
      "metadata": {
        "id": "Xo-77gs2o7gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=Qwen/Qwen2.5-0.5B,dtype=bfloat16,trust_remote_code=True \\\n",
        "  --tasks medmcqa \\\n",
        "  --device cuda:0 \\\n",
        "  --num_fewshot 0 \\\n",
        "  --batch_size auto \\\n",
        "  --output_path results/medmcqa.jsonl\\\n",
        "  --log_samples"
      ],
      "metadata": {
        "id": "Wp-9O0Mqo_rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=llmf25/qwen2_5_0_5b_llmf25_sft_mini,dtype=bfloat16,trust_remote_code=True \\\n",
        "  --tasks medmcqa \\\n",
        "  --device cuda:0 \\\n",
        "  --num_fewshot 0 \\\n",
        "  --batch_size auto \\\n",
        "  --output_path results/medmcqa.jsonl\\\n",
        "  --log_samples"
      ],
      "metadata": {
        "id": "-sxNAZT9pa75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=llmf25/qwen2_5_0_5b_llmf25_sft_mini,dtype=bfloat16,trust_remote_code=True \\\n",
        "  --tasks medqa_4options \\\n",
        "  --device cuda:0 \\\n",
        "  --num_fewshot 0 \\\n",
        "  --batch_size auto \\\n",
        "  --output_path results/medqa.jsonl\\\n",
        "  --log_samples"
      ],
      "metadata": {
        "id": "_n1vtyMTps-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf \\\n",
        "  --model_args pretrained=llmf25/qwen2_5_0_5b_llmf25_sft_mini,dtype=bfloat16,trust_remote_code=True \\\n",
        "  --tasks headqa_en \\\n",
        "  --device cuda:0 \\\n",
        "  --num_fewshot 0 \\\n",
        "  --batch_size auto \\\n",
        "  --output_path results/headqa.jsonl\\\n",
        "  --log_samples"
      ],
      "metadata": {
        "id": "mIQBpoAppxrU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}